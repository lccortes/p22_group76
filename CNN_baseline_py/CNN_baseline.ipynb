{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "\n",
    "from dataset import CellSegmentationDataset\n",
    "from model import UNet\n",
    "from train import train_model\n",
    "from visualize import visualize_predictions\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    data_dir = \"data/brightfield\"  # Update this to your data directory path\n",
    "    dataframe_path = \"data/filename_data.csv\"  # Path to your DataFrame CSV file\n",
    "    wells_train = [2, 3, 4, 5, 6, 7]  # Wells to use for training\n",
    "    wells_val = [1]  # Wells to use for validation\n",
    "    batch_size = 16\n",
    "    epochs = 50\n",
    "    augmentation = True  # Toggle data augmentation\n",
    "    learning_rate = 1e-4\n",
    "    pos_weight = 1.0  # Adjusts how the model weighs positive samples (foreground)\n",
    "    focal_planes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]  # Focal planes to use for training\n",
    "    \n",
    "    # Create a dictionary with the hyperparameters\n",
    "    hyperparameters = {\n",
    "        \"wells_train\": wells_train,\n",
    "        \"wells_val\": wells_val,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": epochs,\n",
    "        \"augmentation\": augmentation,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"pos_weight\": pos_weight,\n",
    "        \"focal_planes\": focal_planes\n",
    "    }\n",
    "    \n",
    "    # Create base results directory at startup\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    base_dir = 'results' + os.sep + timestamp\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    \n",
    "    # Create model checkpoints directory\n",
    "    checkpoint_dir = os.path.join(base_dir, 'model_checkpoints')\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Save hyperparameters to a text file\n",
    "    hyperparameters_txt = os.path.join(base_dir, 'hyperparameters.txt')\n",
    "    with open(hyperparameters_txt, 'w') as f:\n",
    "        for key, value in hyperparameters.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load the DataFrame\n",
    "    df = pd.read_csv(dataframe_path, delimiter=';', index_col=0)\n",
    "\n",
    "    # Image transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        # Add other transformations if needed\n",
    "    ])\n",
    "\n",
    "    # Mask transformations\n",
    "    mask_transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        # You might want to add transforms for masks if necessary\n",
    "    ])\n",
    "\n",
    "    # Datasets and Dataloaders\n",
    "    train_dataset = CellSegmentationDataset(\n",
    "        dataframe=df,\n",
    "        wells=wells_train,\n",
    "        transform=transform,\n",
    "        mask_transform=mask_transform,\n",
    "        augmentation=augmentation,\n",
    "        focal_planes=focal_planes,\n",
    "    )\n",
    "    val_dataset = CellSegmentationDataset(\n",
    "        dataframe=df,\n",
    "        wells=wells_val,\n",
    "        transform=transform,\n",
    "        mask_transform=mask_transform,\n",
    "        augmentation=False,\n",
    "        focal_planes=focal_planes,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Optional: Test the dataset\n",
    "    sample_image, sample_mask = train_dataset[0]\n",
    "    print(f\"Sample image shape: {sample_image.shape}\")  # Should be [N*3, 256, 256]\n",
    "    print(f\"Sample mask shape: {sample_mask.shape}\")    # Should be [1, 256, 256] \n",
    "\n",
    "    # Initialize Model\n",
    "    in_channels = len(focal_planes) * 3  # 3 RGB channels per focal length\n",
    "    model = UNet(in_channels, out_channels=1, dropout_prob=0.6).to(device)\n",
    "\n",
    "    # Save model into a text file\n",
    "    model_txt = os.path.join(base_dir, 'model.txt')\n",
    "    with open(model_txt, 'w') as f:\n",
    "        f.write(str(model))\n",
    "        \n",
    "    # Start Training\n",
    "    trained_model = train_model(model, train_loader, val_loader, epochs, device, pos_weight, learning_rate, base_dir, optimizer='adam')\n",
    "    \n",
    "    # Visualize predictions\n",
    "    visualize_predictions(trained_model, val_dataset, device, num_samples=5, base_dir=base_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
